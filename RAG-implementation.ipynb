{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HamiltonBot\n",
    "\n",
    "This is a RAG-app for the city of Hamilton. This notebook contains the actual RAG code, free of `streamlit` UI components.\n",
    "\n",
    "## Architecture Diagram\n",
    "\n",
    "![diagram](architecture_diagram.png)\n",
    "\n",
    "This notebooks is intended for both myself in the future, employers who just want to see the RAG code, and any future interns who want just the core functionality to improve upon. I opted to use the following packages for this project:\n",
    "- `langchain`: I like the simplicity and elegance the abstractions provide.\n",
    "- `langchain_openai`: As of writing this (January 2024), OpenAI has the current best models.\n",
    "- `chromadb`: Chroma lets me have a local db for storing the embeddings, which simplifies a lot of the security and drives down the cost.\n",
    "- `unstructured`: Given that the RFPs are fairly complex documents with tables and images, I would need a way to parse them into html and base64 formats to feed into the LLMs. (Check issues of tenancy, they say that they don't store, if it's not allowed, try the Hosted SaaS API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.document_loaders import WebBaseLoader\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain_experimental.text_splitter import SemanticChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Transformation\n",
    "\n",
    "As you may know, the first step of any RAG pipeline is to transform the query. This should be done so that the quality and relevance of the documents retrieved can be better. There are a couple techniques for this:\n",
    "1. Rewrite-Retrive-Read: Tell an LLM to try and improve the query by rephrasing it.\n",
    "2. Multi-Query: Have an LLM generate 2-3 queries that ask the same thing different ways.\n",
    "3. Step-back Prompting: Have the LLM to ask some \"more basic\" questions such as asking what principles are being used in the question.\n",
    "4. RAG-Fusion: ???\n",
    "\n",
    "I have currently selected Multi-Query as the Query Transformation. I believe this balances the cost with performance, where Rewrite-Retrieve-Read would be too simple and Step-back Prompting would be too expensive and slow.\n",
    "\n",
    ">ℹ️ The questions that are going to be used to query the vectorstore are not going to be accessible as regular strings, instead they are logs so some code will be required should you want to get the questions as strings.\n",
    "\n",
    "[Article on Query Transformations on the Langchain Blog](https://blog.langchain.dev/query-transformations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Loaders\n",
    "\n",
    "Since the eventual usecase for this system will be QnA with large (200+ page) documents, it's important to chunk them up into more manageable chunks. I would like to experiment (use a different prebuilt function) with Semantic Splitting. Semantic Splitting works by going through the document text 3 consecutive sentences at a time. If the embeddings of two groups of sentences are similar, it will merge both sentences into a single chunk. This way it groups sentences with similar semantic content.\n",
    "\n",
    "[![IMAGE ALT TEXT HERE](https://img.youtube.com/vi/8OJC21T2SL4/0.jpg)](https://www.youtube.com/watch?v=8OJC21T2SL4?si=4s4VROINPiQOWUMh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Method\n",
    "\n",
    "When getting documents from the vectorstore that relate to a certain query, there are a couple options on how to select them.\n",
    "1. The naive approach is the find the return the $k$ most similar embeddings of the documents to the query. This is fine, but for more complex documents (like RFPs), it can be helpful to maximize diversity of the documents.\n",
    "2. Maximal-Marginal Relevance (MMR): This works by finding the embeddings with the greatest cosine similarity to the query but also penalizing them for similarity to already selected documents.\n",
    "\n",
    "Given the nature of RFPs, I will be choosing to use MMR (It's just an option that you can choose from in the `.as_retriever()` method).\n",
    "\n",
    "[Langchain Docs for Selecting Documents with MMR](https://python.langchain.com/docs/modules/model_io/prompts/example_selector_types/mmr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 19, updating n_results = 19\n",
      "Number of requested results 20 is greater than number of elements in index 19, updating n_results = 19\n",
      "Number of requested results 20 is greater than number of elements in index 19, updating n_results = 19\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"to another. But though the borders of these concepts are blurry,\\nthey're not meaningless. I've tried to write about them as precisely\\nas I could without crossing into error.[1]\\nEvolution itself is probably the most pervasive example of\\nsuperlinear returns for performance. But this is hard for us to\\nempathize with because we're not the recipients; we're the returns.[2]\\nKnowledge did of course have a practical effect before the\\nIndustrial Revolution. The development of agriculture changed human\\nlife completely. But this kind of change was the result of broad,\\ngradual improvements in technique, not the discoveries of a few\\nexceptionally learned people.[3]\\nIt's not mathematically correct to describe a step function as\\nsuperlinear, but a step function starting from zero works like a\\nsuperlinear function when it describes the reward curve for effort\\nby a rational actor. If it starts at zero then the part before the\\nstep is below any linearly increasing return, and the part after\\nthe step must be above the necessary return at that point or no one\\nwould bother.[4]\\nSeeking competition could be a good heuristic in the sense that\\nsome people find it motivating. It's also somewhat of a guide to\\npromising problems, because it's a sign that other people find them\\npromising. But it's a very imperfect sign: often there's a clamoring\\ncrowd chasing some problem, and they all end up being trumped by\\nsomeone quietly working on another one.[5]\", metadata={'language': 'No language found.', 'source': 'http://www.paulgraham.com/superlinear.html', 'title': 'Superlinear Returns'}),\n",
       " Document(page_content='Superlinear Returns', metadata={'language': 'No language found.', 'source': 'http://www.paulgraham.com/superlinear.html', 'title': 'Superlinear Returns'}),\n",
       " Document(page_content='Jessica Livingston, Harj Taggar, and Garry Tan for reading drafts\\nof this.', metadata={'language': 'No language found.', 'source': 'http://www.paulgraham.com/superlinear.html', 'title': 'Superlinear Returns'}),\n",
       " Document(page_content=\"so concentrated is thresholds: there's only so much room on the\\nA-list in the average person's head.The most important case combining both sources of superlinear returns\\nmay be learning. Knowledge grows exponentially, but there are also\\nthresholds in it. Learning to ride a bicycle, for example. Some of\\nthese thresholds are akin to machine tools: once you learn to read,\\nyou're able to learn anything else much faster. But the most important\\nthresholds of all are those representing new discoveries. Knowledge\\nseems to be fractal in the sense that if you push hard at the\\nboundary of one area of knowledge, you sometimes discover a whole\\nnew field. And if you do, you get first crack at all the new\\ndiscoveries to be made in it. Newton did this, and so did Durer and\\nDarwin.\\nAre there general rules for finding situations with superlinear\\nreturns? The most obvious one is to seek work that compounds.There are two ways work can compound. It can compound directly, in\\nthe sense that doing well in one cycle causes you to do better in\\nthe next. That happens for example when you're building infrastructure,\\nor growing an audience or brand. Or work can compound by teaching\\nyou, since learning compounds. This second case is an interesting\\none because you may feel you're doing badly as it's happening. You\\nmay be failing to achieve your immediate goal. But if you're learning\", metadata={'language': 'No language found.', 'source': 'http://www.paulgraham.com/superlinear.html', 'title': 'Superlinear Returns'}),\n",
       " Document(page_content=\"How should you take advantage of it?The most obvious way to take advantage of superlinear returns for\\nperformance is by doing exceptionally good work. At the far end of\\nthe curve, incremental effort is a bargain. All the more so because\\nthere's less competition at the far end — and not just for the\\nobvious reason that it's hard to do something exceptionally well,\\nbut also because people find the prospect so intimidating that few\\neven try. Which means it's not just a bargain to do exceptional\\nwork, but a bargain even to try to.There are many variables that affect how good your work is, and if\\nyou want to be an outlier you need to get nearly all of them right.\\nFor example, to do something exceptionally well, you have to be\\ninterested in it. Mere diligence is not enough. So in a world with\\nsuperlinear returns, it's even more valuable to know what you're\\ninterested in, and to find ways to work on it.\\n[9]\\nIt will also be\\nimportant to choose work that suits your circumstances. For example,\\nif there's a kind of work that inherently requires a huge expenditure\\nof time and energy, it will be increasingly valuable to do it when\\nyou're young and don't yet have children.There's a surprising amount of technique to doing great work.\\nIt's not just a matter of trying hard. I'm going to take a shot\\ngiving a recipe in one paragraph.Choose work you have a natural aptitude for and a deep interest in.\\nDevelop a habit of working on your own projects; it doesn't matter\", metadata={'language': 'No language found.', 'source': 'http://www.paulgraham.com/superlinear.html', 'title': 'Superlinear Returns'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "loader = WebBaseLoader(\"http://www.paulgraham.com/superlinear.html\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "    api_key=\"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\",\n",
    "    openai_api_type=\"davinci\",\n",
    ")\n",
    "\n",
    "text_splitter = SemanticChunker(embedding=embedding)\n",
    "chunks = text_splitter.split_documents(docs)\n",
    "\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory=\"./chroma_db\",\n",
    ").from_documents(documents=chunks, embedding=embedding)\n",
    "\n",
    "question = \"What are the fundamental use cases of superlinear returns?\"\n",
    "\n",
    "retriever_from_llm = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 3}),\n",
    "    llm=ChatOpenAI(\n",
    "        temperature=0, api_key=\"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\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "unique_docs = retriever_from_llm.get_relevant_documents(query=question)\n",
    "\n",
    "unique_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "187431cf59a244acb20587bdd08140bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), description='Upload')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "\n",
    "widgets.FileUpload(\n",
    "    accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n",
    "    multiple=False  # True to accept multiple files upload else False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
